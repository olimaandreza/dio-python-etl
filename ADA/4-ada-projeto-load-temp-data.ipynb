{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a474535-6e52-4c5f-aff1-bff1622c2ef1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Projeto desenvolvido por Andreza Lima no âmbito do módulo de Extração de Dados I, na trilha de Engenharia de dados do programa Santander Coders 2023, em parceria com a Ada Tech.\n",
    "\n",
    "Desenvolvido com Databricks Community edition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72d01d8f-872e-44c7-aa88-7383aad97947",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Projeto Sistema de Monitoramento de Avanços no Campo da Genômica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e822efd7-9cad-4f8e-a5d1-e5646c983095",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Contexto:\n",
    "\n",
    "O grupo trabalha no time de engenharia de dados na HealthGen, uma empresa especializada em genômica e pesquisa de medicina personalizada. A genômica é o estudo do conjunto completo de genes de um organismo, desempenha um papel fundamental na medicina personalizada e na pesquisa biomédica. Permite a análise do DNA para identificar variantes genéticas e mutações associadas a doenças e facilita a personalização de tratamentos com base nas características genéticas individuais dos pacientes.\n",
    "\n",
    "A empresa precisa se manter atualizada sobre os avanços mais recentes na genômica, identificar oportunidades para pesquisa e desenvolvimento de tratamentos personalizados e acompanhar as tendências em genômica que podem influenciar estratégias de pesquisa e desenvolvimento. Pensando nisso, o time de dados apresentou uma proposta de desenvolvimento de um sistema que coleta, analisa e apresenta as últimas notícias relacionadas à genômica e à medicina personalizada, e também estuda o avanço do campo nos últimos anos. \n",
    "\n",
    "O time de engenharia de dados tem como objetivo desenvolver e garantir um pipeline de dados confiável e estável. As principais atividades são:\n",
    "\n",
    "1. Consumo de dados com a News API: \n",
    "Implementar um mecanismo para consumir dados de notícias de fontes confiáveis e especializadas em genômica e medicina personalizada, a partir da News API: \n",
    "https://newsapi.org/\n",
    "\n",
    "2. Definir Critérios de Relevância:\n",
    "Desenvolver critérios precisos de relevância para filtrar as notícias. Por exemplo, o time pode se concentrar em notícias que mencionem avanços em sequenciamento de DNA, terapias genéticas personalizadas ou descobertas relacionadas a doenças genéticas específicas.\n",
    "\n",
    "3. Cargas em Batches:\n",
    "Armazenar as notícias relevantes em um formato estruturado e facilmente acessível para consultas e análises posteriores. Essa carga deve acontecer 1 vez por hora. Se as notícias extraídas já tiverem sidos armazenadas na carga anterior, o processo deve ignorar e não armazenar as notícias novamente, os dados carregados não podem ficar duplicados.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1QLZBxgK4c4_yysUnvtamuwXzRJm4nNit\"  width=\"70%\" height=\"40%\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</div>\n",
    "\n",
    "4. Dados transformados para consulta do público final:\n",
    "A partir dos dados carregados, aplicar as seguintes transformações e armazenar o resultado final para a consulta do público final:\n",
    "\n",
    "    4.1 - Quantidade de notícias por ano, mês e dia de publicação;\n",
    "\n",
    "    4.2 - Quantidade de notícias por fonte e autor;\n",
    "    \n",
    "    4.3 - Quantidade de aparições de 3 palavras chaves por ano, mês e dia de publicação (as 3 palavras chaves serão as mesmas usadas para fazer os filtros de relevância do item 2 (2. Definir Critérios de Relevância)).\n",
    "\n",
    "Atualizar os dados transformados 1 vez por dia.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1QOFkzKrWqb-9CY3kC3_1XkTWNVNE05dd\"  width=\"70%\" height=\"40%\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</div>\n",
    "\n",
    "Além das atividades principais, existe a necessidade de busca de dados por eventos em tempo real quando é necessário, para isso foi desenhado duas opções:\n",
    "\n",
    "Opção 1 - Apache Kafka e Spark Streaming:\n",
    "\n",
    "Preparar um pipeline com Apache Kafka e Spark Streaming para receber os dados do Produtor Kafka representado por um evento manual e consumir os dados com o Spark Streaming armazenando os resultados temporariamente. Em um processo paralelo, verificar os resultados armazenados temporiamente e armazenar no mesmo destino do item 3 (3. Cargas em Batches) aqueles resultados que ainda não foram armazenados no destino (os dados carregados não podem ficar duplicados). E por fim, eliminar os dados temporários após a verificação e a eventual carga.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1PvAxBXU0fvwEtJg36ZJ1VfBVSGETBpUZ\"  width=\"70%\" height=\"40%\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "Opção 2 - Webhooks com notificações por eventos:\n",
    "\n",
    "Configurar um webhook para adquirir as últimas notícias a partir de um evento representado por uma requisição POST e fazer a chamada da API e por fim armazenar os resultados temporariamente. Em um processo paralelo, verificar os resultados armazenados temporiamente e armazenar no mesmo destino do item 3 (3. Cargas em Batches) aqueles resultados que ainda não foram armazenados no destino (os dados carregados não podem ficar duplicados). E por fim, eliminar os dados temporários após a verificação e a eventual carga.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Px6Jp3aNuF-wpn_9earonylEMebzOcBW\"  width=\"70%\" height=\"40%\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "</div>\n",
    "\n",
    "## Atividades que precisam ser realizadas pelo grupo:\n",
    "\n",
    "O grupo precisa construir o pipeline de dados seguindo os requisitos das atividades principais e escolher entre a Opção 1 e Opção 2 para desenvolvimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f21d5970-c76d-4285-984c-19c80609cfae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Proposta de Resolução\n",
    "\n",
    "A execução deste projeto foi dividida em 5 arquivos.\n",
    "\n",
    "1-ada-projeto-load-data: contém os passos necessários para carregamento dos dados de hora em hora\n",
    "\n",
    "2-ada-projeto-transform-data: contém os passos necessários para transformação do arquivo consolidado, a cada 24 horas\n",
    "\n",
    "3-ada-projeto-webhook: cria uma aplicação flask para o processo de webhook\n",
    "\n",
    "4-ada-projeto-load-temp-data (este arquivo): contém os passos necessários para carregamento dos dados em uma pasta temporária, e depois carregamento no arquivo principal\n",
    "\n",
    "5-ada-projeto-webhook-post: envia post commands para a aplicação criada no arquivo 3-ada-projeto-webhook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9025ae3d-0ad4-4992-855c-6d281f44a499",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Importando Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e06e2af8-448e-4b9f-8120-d7b548249e37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Executar apenas se for necessário instalar pacotes\n",
    "# !pip install pyspark\n",
    "# !pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b782f1b-4496-432c-9921-68f099b4182e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import flask\n",
    "import json\n",
    "import pyspark.pandas as ps\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "757e4309-107b-4e1a-bd7d-8032a1de9865",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Consumo de dados com a News API\n",
    "Ir para o arquivo 1-ada-projeto-load-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8b07f9a-7499-47d9-87f2-34f3c22e4c6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Definir Critérios de Relevância\n",
    "\n",
    "De acordo com Antunes (2022)¹, após cerca de 20 anos depois da publicação do primeiro rascunho da sequência de todo o genoma humano, os investigadores da área mergulharam na era “pós-genómica”, remodelando a investigação biológica e abrindo portas para a chamada medicina personalizada ou medicina de precisão, que pesquisa como medicamentos podem ser adaptados de um modo preciso, proporcionando o melhor tratamento possível através da sequenciação do genoma de cada indivíduo.\n",
    "\n",
    "Tendo como base essa discussão sobre os avanços da genômica, as palavras-chave escolhidas para este projeto foram os termos em inglês *genomics*, *precision medicine* e *personalized medicine*.\n",
    "\n",
    "¹ Antunes, A., (2022) Avanços da genómica, Rev. Ciência Elem., V10(4):056. Disponível em: https://rce.casadasciencias.org/rceapp/art/2022/056/. Acesso em: 25 set. 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ad37526-4512-4efe-bcee-d38c219f08d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Cargas em Batches:\n",
    "\n",
    "Ir para o arquivo 1-ada-projeto-load-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74efc029-dfde-401d-a0ed-b3eb6f5e06b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Opção 2 - Webhooks com notificações por eventos:\n",
    "\n",
    "Webhook é uma forma de automação de comunicação entre sistemas. É um método que permite que um sistema envie automaticamente dados ou informações para outro sistema assim que um evento específico ocorrer. \n",
    "\n",
    "Para fins deste projeto, será criada uma aplicação com flask para simular um evento que ocorre sempre que um post command é executado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3d06b5-58e8-4d9e-a95d-03d8534e348b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Criação de uma aplicação flask para simular um evento\n",
    "\n",
    "Configurar um webhook para adquirir as últimas notícias a partir de um evento representado por uma requisição POST e fazer a chamada da API e por fim armazenar os resultados temporariamente. Em um processo paralelo, verificar os resultados armazenados temporiamente e armazenar no mesmo destino do item 3 (3. Cargas em Batches) aqueles resultados que ainda não foram armazenados no destino (os dados carregados não podem ficar duplicados). E por fim, eliminar os dados temporários após a verificação e a eventual carga.\n",
    "\n",
    "Ir para o arquivo 3-ada-projeto-webhook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e30e2c43-914f-4491-86ff-af58109f3088",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Carregamento de dados temporários\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5cde674-51a2-4c50-b3e7-de767ed10930",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Função para carregar e processar os dados temporários obtidos a partir do webhook\n",
    "def load_temp(dir_leitura, dir_escrita_bruto):\n",
    "    \"\"\"\n",
    "    Função que recebe um path de diretório de leitura de arquivos temporários e um path de um arquivo a ser consolidado\n",
    "    Ação: Lê cada arquivo no diretório de leitura e executa o load_data desses arquivos no\n",
    "    Saída: None\n",
    "    \"\"\"\n",
    "    try: # verifica se existe novos dados para serem processados\n",
    "        arquivo = dbutils.fs.ls(dir_leitura)\n",
    "        if arquivo == []:\n",
    "            print(\"Nenhum dado novo\")\n",
    "        else:\n",
    "            print(\"\\n> Arquivos novos encontrados\")\n",
    "        for arquivo in dbutils.fs.ls(dir_leitura):\n",
    "            #lê o arquivo e faz a chamada da carga\n",
    "            print(\"\\n> Leitura do arquivo: \" + arquivo[1])\n",
    "            df_new = ps.read_parquet(dir_leitura+arquivo[1])\n",
    "            load_data(df_new, dir_escrita_bruto)\n",
    "            print(\"\\n> \" + arquivo[1] + \" carregado com sucesso.\" )\n",
    "                \n",
    "            #realiza a movimentação da pasta de dados brutos para a pasta de dados brutos já carregados\n",
    "            print(\"Removendo arquivo temporário.\")\n",
    "            dbutils.fs.rm(dir_leitura+arquivo[1],True)\n",
    "\n",
    "    except Exception as e: #caso não exista nenhum dado novo, retorna com a mensagem e encerra o processo\n",
    "        if 'java.io.FileNotFoundException' in str(e):\n",
    "            print(\"Nenhum dado novo\")\n",
    "        else:\n",
    "            print(\"erro no ELT:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aa1cfc9-5ea6-4f75-9dde-57a0ca4c9a56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definindo variáveis para o load_temp\n",
    "\n",
    "tempo_proc_load_temp = 1* 60 # * 60 # 1 hora * 60 minutos * 60 segundos\n",
    "dir_leitura = \"/FileStore/projeto/temp/\"\n",
    "dir_escrita_bruto = \"/FileStore/projeto/dados_consolidados.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c12ca3-7d85-462a-a57b-62e2db555c50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n2023-09-29 20:13:09: Próxima verificação em 60 segundos.\nIniciando carga dos dados temporários\nNenhum dado novo\n\n2023-09-29 20:14:09: Próxima verificação em 60 segundos.\nIniciando carga dos dados temporários\n\n> Arquivos novos encontrados\n\n> Leitura do arquivo: dados_tmp_2023-09-29 20:14:22.parquet/\n\nIniciando carregamento de dados.\n\nArquivo já existe. Iniciando leitura do arquivo existente.\n\nFim da leitura do arquivo existente.\n\nIniciando concatenação do novo arquivo.\n\nNovo arquivo concatenado com sucesso.\n\nRemovendo duplicados do arquivo\n\nIniciando salvamento do novo arquivo.\n\nResultado carregado com sucesso\n\n> dados_tmp_2023-09-29 20:14:22.parquet/ carregado com sucesso.\nRemovendo arquivo temporário.\n\n> Leitura do arquivo: dados_tmp_2023-09-29 20:14:32.parquet/\n\nIniciando carregamento de dados.\n\nArquivo já existe. Iniciando leitura do arquivo existente.\n\nFim da leitura do arquivo existente.\n\nIniciando concatenação do novo arquivo.\n\nNovo arquivo concatenado com sucesso.\n\nRemovendo duplicados do arquivo\n\nIniciando salvamento do novo arquivo.\n\nResultado carregado com sucesso\n\n> dados_tmp_2023-09-29 20:14:32.parquet/ carregado com sucesso.\nRemovendo arquivo temporário.\n\n2023-09-29 20:15:27: Próxima verificação em 60 segundos.\nIniciando carga dos dados temporários\nNenhum dado novo\n\n2023-09-29 20:16:27: Próxima verificação em 60 segundos.\nIniciando carga dos dados temporários\nNenhum dado novo\n\n2023-09-29 20:17:27: Próxima verificação em 60 segundos.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loop chamando o transform_data a cada dia\n",
    "\n",
    "contador_load_temp = 0 # Limitar para testes\n",
    "while contador_load_temp < 30:\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(\"\\n\" + timestamp+ f\": Próxima verificação em {tempo_proc_load_temp} segundos.\")\n",
    "\n",
    "    time.sleep(tempo_proc_load_temp)\n",
    "    print(\"Iniciando carga dos dados temporários\")\n",
    "    load_temp(dir_leitura, dir_escrita_bruto)\n",
    "    contador_load_temp +=1 # Comentar esta linha para rodar infinitamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "426f6d04-d46f-4af9-8435-54275b2bc8d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Execução do post command\n",
    "Ir para o arquivo 5-ada-projeto-webhook-post"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1958246452001375,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4-ada-projeto-load-temp-data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
